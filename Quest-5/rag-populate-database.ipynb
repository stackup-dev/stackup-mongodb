{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Populate MongoDB Atlas Database\n",
    "In this Python notebook, we will be generating embeddings for two 10K filing documents, indexing them and storing them into MongoDB Atlas database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n",
    "We'll start with some basic setup steps in the next two code cells. Don't worry if your device does not have a GPU, you will still be able to proceed with the Quest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA/GPU:  False\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is enabled\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# To disable GPU and experiment, uncomment the following line\n",
    "# Normally, you would want to use GPU, if one is available\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging. To see more logging, set the level to DEBUG\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Inspecting the Documents\n",
    "\n",
    "As mentioned earlier in the quest, we'll be using two 10K filings as our corpus. These are financial documents filed by US public companies to SEC (Securities and Exchange Commission). You can read about them [here](https://www.investor.gov/introduction-investing/investing-basics/glossary/form-10-k).\n",
    "\n",
    "The two 10K filings we'll be using are Lyft and Uber. You can find these documents in the `data` folder:\n",
    "```text\n",
    "data/10k\n",
    "├── lyft_2021.pdf\n",
    "└── uber_2021.pdf\n",
    "```\n",
    "\n",
    "Don't be fooled by the fact that it is just 2 documents - each PDF document is over 200+ pages long! These aren't any ordinary simple text documents, these are very dense text documents containing a lot of very specific information. \n",
    "\n",
    "Go ahead and inspect these documents manually before proceeding with this notebook to get a feel for the kinds of documents we're dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS_URI Connection string found: mongodb+srv://yongtaufoo:mucjOuDXLysFfEGA@cluster0.ds8hjdi.mongodb.net/?retryWrites=true&w=majority\n"
     ]
    }
   ],
   "source": [
    "# Load settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# Change system path to root direcotry\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# For debugging purposes\n",
    "# print (config)\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "else:\n",
    "    print(\"ATLAS_URI Connection string found:\", ATLAS_URI)\n",
    "\n",
    "## Only uncomment this if you are using OpenAI for embeddings\n",
    "# OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "# if not OPENAI_API_KEY:\n",
    "#     raise Exception (\"'OPENAI_API_KEY' is not set. Please set it above to continue...\")\n",
    "# else:\n",
    "#     print(\"ATLAS_URI Connection string found:\", ATLAS_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our variables\n",
    "DB_NAME = 'rag1'\n",
    "COLLECTION_NAME = '10k'\n",
    "INDEX_NAME = 'idx_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex will download embeddings models as needed\n",
    "# Set llamaindex cache dir to ../cache dir here (Default is system tmp)\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(ATLAS_URI)\n",
    "\n",
    "print (\"Atlas client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clear out the collection\n",
    "\n",
    "In the event you're repeating this Quest again, run the code cell below for a fresh start! This code cell will delete the documents you've previously added to the `rag1 - 10k` collection (if any). If you are running this Quest for the first time, you should expect to see `0` below because you've yet to add any items into this collection yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count before delete : 754\n",
      "Deleted docs : 754\n"
     ]
    }
   ],
   "source": [
    "database = mongodb_client[DB_NAME]\n",
    "collection = database [COLLECTION_NAME]\n",
    "\n",
    "doc_count = collection.count_documents (filter = {})\n",
    "print (f\"Document count before delete : {doc_count:,}\")\n",
    "\n",
    "result = collection.delete_many(filter= {})\n",
    "print (f\"Deleted docs : {result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup Embeddings\n",
    "\n",
    "Now, we'll need to establish an embedding model to help us generate embeddings for the documents we'll be uploading. Here, we provide two options - the first option is to use an OpenAI embedding model and the second option is to use an open source HuggingFace embedding model. We'll be going with the second approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Option A: OpenAI Embeddings\n",
    "\n",
    "This option utilizes an OpenAI embedding model. As such, you will need to have an OpenAI API key (as defined in env variable `OPENAI_API_KEY`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only uncomment this if you are using OpenAI for Embeddings\n",
    "# from llama_index import  OpenAIEmbedding\n",
    "# embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Option B : Using Custom Embeddings\n",
    "\n",
    "This option utilizes a HuggingFace embedding model. Recap from Quests 3 and 4 that you can choose from a large number of options. Listed below are some examples, taken from the leaderboard https://huggingface.co/spaces/mteb/leaderboard. We'll be going with the `BAAI/bge-small-en-v.15` embedding model here.\n",
    "\n",
    "| model name                              | overall score | model size | model params | embedding length | License  | url                                                            |\n",
    "|-----------------------------------------|---------------|------------|--------------|------------------|----------|----------------------------------------------------------------|\n",
    "| BAAI/bge-large-en-v1.5                  | 64.x          | 1.34 GB    | 335 M        | 1024             | MIT      | https://huggingface.co/BAAI/bge-large-en-v1.5                  |\n",
    "| BAAI/bge-small-en-v1.5                  | 62.x          | 133 MB     | 33.5 M       | 384              | MIT      | https://huggingface.co/BAAI/bge-small-en-v1.5                  |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 57.8          | 438 MB     |              | 768              | Apache 2 | https://huggingface.co/sentence-transformers/all-mpnet-base-v2 |\n",
    "| sentence-transformers/all-MiniLM-L12-v2 | 56.x          | 134 MB     |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 56.x          | 91 MB      |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james.lim/anaconda3/envs/atlas-1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|█████████████████████████████████████████████████████████| 743/743 [00:00<00:00, 1.22MB/s]\n",
      "model.safetensors: 100%|█████████████████████████████████████████████████| 133M/133M [00:05<00:00, 23.0MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████| 366/366 [00:00<00:00, 579kB/s]\n",
      "vocab.txt: 100%|██████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 557kB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████| 711k/711k [00:00<00:00, 745kB/s]\n",
      "special_tokens_map.json: 100%|██████████████████████████████████████████████| 125/125 [00:00<00:00, 276kB/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "## Set up embedding model\n",
    "# The LLM used to generate natural language responses to queries\n",
    "# If not provided, it will default to gpt-3.5-turbo from OpenAI\n",
    "# If your OpenAI API key is not set, it will default to llama2-chat-13B from Llama.cpp\n",
    "# Since we don't need an LLM just yet, we'll be setting it to None\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Connect Llama-Index and MongoDB Atlas\n",
    "\n",
    "As mentioned in the Quest, we'll be using MongoDB Atlas as our vector storage. This is critical to store indexed data and then query later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = 'idx_embedding',\n",
    "                                 ## The following columns are set to default values\n",
    "                                 # embedding_key = 'embedding', text_key = 'text', metadata_= 'metadata',\n",
    "                                 )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Read PDF Documents\n",
    "\n",
    "Llama-index has very handy `SimpleDirectoryReader` that can read single/multiple files and also an entire directory's content. We'll be using this to read our 2 PDF files and storing the data in `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 545 chunks from '../data/10k/'\n",
      "CPU times: user 6.33 s, sys: 57.3 ms, total: 6.38 s\n",
      "Wall time: 6.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.readers.file.base import SimpleDirectoryReader\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "data_dir = '../data/10k/'\n",
    "\n",
    "## This reads one doc\n",
    "# docs = SimpleDirectoryReader(\n",
    "#     input_files=[\"./data/10k/uber_2021.pdf\"]\n",
    "# ).load_data()\n",
    "\n",
    "## This reads an entire directory\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Index the Documents and Store Them Into MongoDB Atlas\n",
    "\n",
    "The code cell below is where everything that we've been preparing for in this Python notebook comes together:\n",
    "- Embeddings are generated using our packaged-up embedding model `service_context` \n",
    "- Our documents `docs` get indexed `storage_context` - both text and embeddings are stored in MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.05 s, sys: 1.21 s, total: 5.27 s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, \n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code cell above, there should be a new database `rag1` and a new collection `10k` inside it that contains the text as well as the generated embeddings of our 2 PDF files.\n",
    "\n",
    "And we're done with this notebook! Please **head back to the Quest page on StackUp now** where we'll be checking to see if we've indeed managed to successfully add the data to our MongoDB Atlas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-1",
   "language": "python",
   "name": "atlas-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
