{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Embeddings\n",
    "In this Python notebook, we will try running embedding models locally and compare their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama-Index\n",
    "We'll be making use of LlamaIndex to help us download the embedding models from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you face issues running the code cell below, run this code cell to install some libraries\n",
    "\n",
    "# !pip install llama-index-llms-huggingface\n",
    "# !pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james.lim/anaconda3/envs/atlas-1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████████████████████████| 743/743 [00:00<00:00, 939kB/s]\n",
      "model.safetensors: 100%|█████████████████████| 133M/133M [00:09<00:00, 14.2MB/s]\n",
      "tokenizer_config.json: 100%|███████████████████| 366/366 [00:00<00:00, 2.41MB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 8.23MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 711k/711k [00:00<00:00, 16.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████████████| 125/125 [00:00<00:00, 331kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.003275729948654771, -0.011690844781696796, 0.04155920818448067, -0.03814816102385521, 0.02418307028710842]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Optional, but set llamaindex cache dir to ../cache dir here. Default is system tmp\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')\n",
    "\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! As you can see from the output above, we managed to download the `BAAI/bge-small-en-v1.5` model from Hugging Face by using Llama-Index and using the downloaded model to get the vector representation of the string \"Hello World!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Let's Try a Few Embedding Models\n",
    "As mentioned earlier in the quest, Hugging Face has many different models available for us to choose from. In this quest, we'll be using the following three models:\n",
    "\n",
    "| model name                              | overall score | model params | model size | embedding length | url                                                            |\n",
    "|-----------------------------------------|---------------|--------------|------------|------------------|----------------------------------------------------------------|\n",
    "| BAAI/bge-small-en-v1.5                  | 62.x          | 33.5 M       | 133 MB     | 384              | https://huggingface.co/BAAI/bge-small-en-v1.5                  |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 57.8          |              | 438 MB     | 768              | https://huggingface.co/sentence-transformers/all-mpnet-base-v2 |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 56.x          |              | 91 MB      | 384              | https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  |\n",
    "\n",
    "If you would like to, you can also download other models provided by [Hugging Face](https://huggingface.co/spaces/mteb/leaderboard) and compare the results you get from running them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "What we're doing in the code cell below is running a benchmark test on our three chosen models. Essentially, we're downloading the models into our local machines, using them to create embeddings for the string \"Hello World!\" and comparing the speed of each model when it comes to generating embeddings.\n",
    "\n",
    "Don't worry if you see the `TqdmWarning: IProgress not found` warning message, it should not affect your code from running.\n",
    "\n",
    "After running the code cell below, if you look in the `cache` folder of the root directory of your project, you should be able to see that you've downloaded these three embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=BAAI/bge-small-en-v1.5, embeding_length=384\n",
      "16.5 ms ± 589 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "model=sentence-transformers/all-mpnet-base-v2, embeding_length=768\n",
      "19.3 ms ± 488 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "model=sentence-transformers/all-MiniLM-L6-v2, embeding_length=384\n",
      "9.73 ms ± 189 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import timeit\n",
    "\n",
    "embedding_models = [\n",
    "    'BAAI/bge-small-en-v1.5',\n",
    "    'sentence-transformers/all-mpnet-base-v2',\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "for model in embedding_models:\n",
    "    embed_model = HuggingFaceEmbedding(model_name=model)\n",
    "    embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "    print(f'model={model}, embeding_length={len(embeddings):,}')\n",
    "    %timeit (embed_model.get_text_embedding(\"Hello World!\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL] Using SentenceTransformers\n",
    "Note that besides using Llama-Index, you can also use other providers like SentenceTransformers to download embedding models; the code cell below demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      ".gitattributes: 100%|██████████████████████████████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 3.29MB/s]\n",
      "1_Pooling/config.json: 100%|████████████████████████████████████████████████████████████████████████████| 190/190 [00:00<00:00, 626kB/s]\n",
      "README.md: 100%|███████████████████████████████████████████████████████████████████████████████████| 10.6k/10.6k [00:00<00:00, 11.7MB/s]\n",
      "config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████| 612/612 [00:00<00:00, 1.96MB/s]\n",
      "config_sentence_transformers.json: 100%|████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 425kB/s]\n",
      "data_config.json: 100%|████████████████████████████████████████████████████████████████████████████| 39.3k/39.3k [00:00<00:00, 19.6MB/s]\n",
      "pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████| 90.9M/90.9M [00:06<00:00, 14.2MB/s]\n",
      "sentence_bert_config.json: 100%|██████████████████████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 196kB/s]\n",
      "special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 156kB/s]\n",
      "tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 8.53MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████| 350/350 [00:00<00:00, 503kB/s]\n",
      "train_script.py: 100%|█████████████████████████████████████████████████████████████████████████████| 13.2k/13.2k [00:00<00:00, 20.3MB/s]\n",
      "vocab.txt: 100%|██████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 457kB/s]\n",
      "modules.json: 100%|█████████████████████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 758kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Normalize()\n",
      ")\n",
      "384\n",
      "[-0.00542394  0.07206918 -0.02727438  0.04371348 -0.06957784]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# embeddings = model.encode(sentences)\n",
    "embeddings = model.encode('a happy dog!')\n",
    "print(model)\n",
    "print (len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done with this notebook! Please **head back to the Quest page on StackUp now**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-1",
   "language": "python",
   "name": "atlas-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
